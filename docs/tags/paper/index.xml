<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper on tsumli-pages</title>
    <link>http://localhost:1313/tags/paper/</link>
    <description>Recent content in Paper on tsumli-pages</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <copyright>© {year}</copyright>
    <lastBuildDate>Tue, 06 Sep 2022 10:33:39 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation を読む</title>
      <link>http://localhost:1313/blog/paper/bisenet/</link>
      <pubDate>Tue, 06 Sep 2022 10:33:39 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/bisenet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1808.00897&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Yu et al. ECCV 2018] という論文を読んでいきます。 (本文中の図は論文より引用) 。&#xA;semantic segmentationの課題である、リアルタイム性 (推論速度) と精度のトレードオフ&#xA;を解決する新たなネットワークBilateral Segmentation Network (BiSeNet) の提案。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lagrangian Fluid Simulation With Continuous Convolutions を読む</title>
      <link>http://localhost:1313/blog/paper/deeplagrangianfluids/</link>
      <pubDate>Fri, 08 Jul 2022 15:55:54 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/deeplagrangianfluids/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://openreview.net/pdf?id=B1lDoJSYDH&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;Lagrangian Fluid Simulation&#xA;With Continuous Convolutions&lt;/strong&gt;&lt;/a&gt;&#xA; という論文を読んでいきます (本文中の図は論文より引用) 。&#xA;この論文では液体をグラフとして扱う&amp;hellip;のではなくspatial convolutionを使って近傍の粒子との関係を計算します。&lt;/p&gt;</description>
    </item>
    <item>
      <title>First Order Motion Model for Image Animation (FOMM) を読む</title>
      <link>http://localhost:1313/blog/paper/fomm/</link>
      <pubDate>Tue, 01 Feb 2022 19:58:16 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/fomm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;First Order Motion Model for Image Animation&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[A. Siarohin et al. NeurIPS 2019] という論文を読んでいきます (本文中の図は論文より引用)。&#xA;FOMMという名前で知られている手法です。&lt;/p&gt;&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/result.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/fomm/images/result.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/fomm/images/result.png&#34;&#xA;      &#xA;        alt=&#34;example animation&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . example animation&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;&#xA;&lt;h3 id=&#34;目的&#34;&gt;目的&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;source image $\mathbf{S}$ と deiving video $\mathcal{D}$ の動きを表す潜在表現を組み合わせて、 driving video を再構成する&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Video Swin Transformer を読む</title>
      <link>http://localhost:1313/blog/paper/video-swin-transformer/</link>
      <pubDate>Thu, 14 Oct 2021 15:55:54 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/video-swin-transformer/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.13230&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;Video Swin Transformer&lt;/strong&gt;&lt;/a&gt;&#xA;という論文を読んでいきます (本文中の図は論文より引用) 。ざっくり言うと、Swin Transformerをそのままvideoの入力に拡張した論文です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む</title>
      <link>http://localhost:1313/blog/paper/swin-transformer/</link>
      <pubDate>Thu, 14 Oct 2021 13:55:54 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/swin-transformer/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.14030&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/strong&gt;&lt;/a&gt;&#xA;という論文を読んでいきます (本文中の図は論文より引用) 。&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/compare_vit.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/swin-transformer/images/compare_vit.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/swin-transformer/images/compare_vit.png&#34;&#xA;      &#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;&#xA;&lt;p&gt;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/architecture.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/swin-transformer/images/architecture.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/swin-transformer/images/architecture.png&#34;&#xA;      &#xA;        alt=&#34;The architecture of a Swin Transformer (b) two successive Swin Transformer Blocks.&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure 1. The architecture of a Swin Transformer (b) two successive Swin Transformer Blocks.&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;Swin Transformerの全体像 (小さいバージョン)。&#xA;まず、RGB画像をオーバーラップしないようにpatchに分ける。そして、そのRGB値をconcatしたものが特徴量として扱われる。&#xA;この論文では、$4\times 4$ のpatchに分けている。つまり、 特徴量は $4\times 4 \times 3 = 48$ 次元となる。&#xA;この特徴量は線形層に通され、任意の次元に投影される。&lt;/p&gt;</description>
    </item>
    <item>
      <title>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing を読む</title>
      <link>http://localhost:1313/blog/paper/one-shot-talking-head/</link>
      <pubDate>Wed, 13 Oct 2021 13:55:54 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/one-shot-talking-head/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.15126&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing&lt;/strong&gt;&lt;/a&gt;&#xA;という論文を読んでいきます。CVPR 2021にacceptされています (本文中の図は論文より引用) 。&lt;/p&gt;&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/overview.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/one-shot-talking-head/images/overview.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/one-shot-talking-head/images/overview.png&#34;&#xA;      &#xA;        alt=&#34;提案手法の結果。H.264より10倍以上効率的にビデオを圧縮できる。また、画像のposeを自由に変更することができる。&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . 提案手法の結果。H.264より10倍以上効率的にビデオを圧縮できる。また、画像のposeを自由に変更することができる。&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;&#xA;&lt;p&gt;Contrubutionは次の通り&lt;/p&gt;</description>
    </item>
    <item>
      <title>TGAN: Synthesizing Tabular Data using Generative Adversarial Networks を読む</title>
      <link>http://localhost:1313/blog/paper/TGAN/</link>
      <pubDate>Mon, 14 Jun 2021 10:33:39 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/TGAN/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1811.11264&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;Synthesizing Tabular Data using Generative Adversarial Networks&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Xu et al. arXiv 2018] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p&gt;&#xA;&lt;p&gt;tabular dataに対するGAN (TGAN) についての紹介.&#xA;様々なデータ (categorical, numericalなど) が混合したようなテーブルに対して用いることができる.&lt;/p&gt;</description>
    </item>
    <item>
      <title>FNet: Mixing Tokens with Fourier Transforms を読む</title>
      <link>http://localhost:1313/blog/paper/FNet/</link>
      <pubDate>Thu, 03 Jun 2021 07:11:44 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/FNet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2105.03824&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Thorp et al. arXiv 2020] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p&gt;&#xA;&lt;p&gt;Contributions:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;token &amp;ldquo;mixing&amp;rdquo; 変換がテキストデータにおける多様なsemanticsを十分に捉えられることを示した&lt;/li&gt;&#xA;&lt;li&gt;self-attention層をFourier Transform層に置き換えたTransformer-likeな &lt;strong&gt;FNet&lt;/strong&gt; というモデルの提案&lt;/li&gt;&#xA;&lt;li&gt;学習速度が早く (TPUでは短いシークエンスのときのみ), 精度も良い. また, メモリ使用量も比較的少なくすむ&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;&#xA;&lt;h3 id=&#34;background-discrete-fourier-transforms&#34;&gt;Background: discrete Fourier Transforms&lt;/h3&gt;&#xA;&lt;p&gt;シークエンス $\lbrace x_n \rbrace$ $\left( n \in [0, N-1] \right)$ が与えられたとき,&#xA;discrete Fourier Transform (DFT) は次のように表される.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GANimation を読む</title>
      <link>http://localhost:1313/blog/paper/GANimation/</link>
      <pubDate>Sat, 22 May 2021 11:38:22 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/GANimation/</guid>
      <description>&lt;!-- define macro  --&gt;&#xA;&lt;p&gt;$$&#xA;\def\image#1{\mathbf{I}_{\mathbf{y}{\scriptsize{#1}}}}&#xA;$$&lt;/p&gt;&#xA;&lt;!-- end define --&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.09251&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;GANimation: Anatomically-aware Facial Animation from a Single Image&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Pumarola et al. ECCV 2018] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p&gt;&#xA;&lt;p&gt;Action Units (AU) アノテーションに基づいたGAN&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/synthesis_result.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/GANimation/images/synthesis_result.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/GANimation/images/synthesis_result.png&#34;&#xA;      &#xA;        alt=&#34;Facial animation from a single image.&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . Facial animation from a single image.&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>StarGAN v2 を読む</title>
      <link>http://localhost:1313/blog/paper/StarGANv2/</link>
      <pubDate>Fri, 21 May 2021 19:58:16 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/StarGANv2/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.01865&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;StarGAN v2: Diverse Image Synthesis for Multiple Domains&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Choi et al. CVPR 2020] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p&gt;&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/Synth_result.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/StarGANv2/images/Synth_result.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/StarGANv2/images/Synth_result.png&#34;&#xA;      &#xA;        alt=&#34;Synthesis Result&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . Synthesis Result&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;image-to-image translationのタスクにおいて, 異なるドメイン間のマッピングを学習しなければならない.&#xA;そして, それらは次の要素を満たす必要がある.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Taskonomyを読む</title>
      <link>http://localhost:1313/blog/paper/taskonomy/</link>
      <pubDate>Mon, 01 Feb 2021 13:55:54 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/taskonomy/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1804.08328&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;Taskonomy: Disentangling Task Transfer Learning&lt;/strong&gt;&lt;/a&gt;&#xA;という論文を読んでいきます。&#xA;この論文はCVPR 2018のBestPaperを受賞しています (本文中の図は論文より引用) 。&lt;/p&gt;&#xA;&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;&#xA;&lt;p&gt;タスク間の転移学習しやすさが分かれば、アノテーションの足りないデータを扱う、または性能を向上させたいときにどのタスクで事前学習を行うべきかが分かる。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
