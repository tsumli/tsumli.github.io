<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GAN on tsumli-pages</title>
    <link>http://localhost:1313/tags/gan/</link>
    <description>Recent content in GAN on tsumli-pages</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <copyright>© {year}</copyright>
    <lastBuildDate>Tue, 01 Feb 2022 19:58:16 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/gan/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>First Order Motion Model for Image Animation (FOMM) を読む</title>
      <link>http://localhost:1313/blog/paper/fomm/</link>
      <pubDate>Tue, 01 Feb 2022 19:58:16 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/fomm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;First Order Motion Model for Image Animation&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[A. Siarohin et al. NeurIPS 2019] という論文を読んでいきます (本文中の図は論文より引用)。&#xA;FOMMという名前で知られている手法です。&lt;/p&gt;&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/result.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/fomm/images/result.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/fomm/images/result.png&#34;&#xA;      &#xA;        alt=&#34;example animation&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . example animation&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;&#xA;&lt;h3 id=&#34;目的&#34;&gt;目的&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;source image $\mathbf{S}$ と deiving video $\mathcal{D}$ の動きを表す潜在表現を組み合わせて、 driving video を再構成する&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing を読む</title>
      <link>http://localhost:1313/blog/paper/one-shot-talking-head/</link>
      <pubDate>Wed, 13 Oct 2021 13:55:54 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/one-shot-talking-head/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.15126&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing&lt;/strong&gt;&lt;/a&gt;&#xA;という論文を読んでいきます。CVPR 2021にacceptされています (本文中の図は論文より引用) 。&lt;/p&gt;&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/overview.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/one-shot-talking-head/images/overview.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/one-shot-talking-head/images/overview.png&#34;&#xA;      &#xA;        alt=&#34;提案手法の結果。H.264より10倍以上効率的にビデオを圧縮できる。また、画像のposeを自由に変更することができる。&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . 提案手法の結果。H.264より10倍以上効率的にビデオを圧縮できる。また、画像のposeを自由に変更することができる。&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;&#xA;&lt;p&gt;Contrubutionは次の通り&lt;/p&gt;</description>
    </item>
    <item>
      <title>GANimation を読む</title>
      <link>http://localhost:1313/blog/paper/GANimation/</link>
      <pubDate>Sat, 22 May 2021 11:38:22 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/GANimation/</guid>
      <description>&lt;!-- define macro  --&gt;&#xA;&lt;p&gt;$$&#xA;\def\image#1{\mathbf{I}_{\mathbf{y}{\scriptsize{#1}}}}&#xA;$$&lt;/p&gt;&#xA;&lt;!-- end define --&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.09251&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;GANimation: Anatomically-aware Facial Animation from a Single Image&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Pumarola et al. ECCV 2018] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p&gt;&#xA;&lt;p&gt;Action Units (AU) アノテーションに基づいたGAN&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/synthesis_result.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/GANimation/images/synthesis_result.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/GANimation/images/synthesis_result.png&#34;&#xA;      &#xA;        alt=&#34;Facial animation from a single image.&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . Facial animation from a single image.&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>StarGAN v2 を読む</title>
      <link>http://localhost:1313/blog/paper/StarGANv2/</link>
      <pubDate>Fri, 21 May 2021 19:58:16 +0000</pubDate>
      <guid>http://localhost:1313/blog/paper/StarGANv2/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.01865&#34; &#xA;  &#xA;   target=&#34;_blank&#34; rel=&#34;noreferrer noopener&#34; &#xA;&gt;&lt;strong&gt;StarGAN v2: Diverse Image Synthesis for Multiple Domains&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;br&gt;&#xA;[Choi et al. CVPR 2020] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p&gt;&#xA;&#xA;  &#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;figure&#xA;  &#xA;  &#xA;  &#xA;  style=&#39;margin:0 auto;text-align:center;&#39;&#xA;  &gt;&#xA;    &lt;a &#xA;      &#xA;        data-lightbox=&#34;image-images/Synth_result.png&#34;&#xA;      &#xA;      &#xA;        href=&#34;http://localhost:1313/blog/paper/StarGANv2/images/Synth_result.png&#34;&#xA;      &#xA;    &#xA;    &gt;&#xA;  &lt;img&#xA;      &#xA;        src=&#34;http://localhost:1313/blog/paper/StarGANv2/images/Synth_result.png&#34;&#xA;      &#xA;        alt=&#34;Synthesis Result&#34;&#xA;        &#xA;        &#xA;         /&gt;&#xA;    &lt;/a&gt;&#xA;  &#xA;  &#xA;    &lt;figcaption&gt;&#xA;      &lt;span class=&#34;img--caption&#34;&gt;&#xA;        Figure . Synthesis Result&#xA;        &#xA;      &lt;/span&gt;&#xA;    &lt;/figcaption&gt;&#xA;  &#xA;&lt;/figure&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;p&gt;image-to-image translationのタスクにおいて, 異なるドメイン間のマッピングを学習しなければならない.&#xA;そして, それらは次の要素を満たす必要がある.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
