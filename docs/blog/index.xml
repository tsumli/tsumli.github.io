<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blogs on tsumli-pages</title><link>https://tsumli.github.io/blog/</link><description>Recent content in Blogs on tsumli-pages</description><generator>Hugo</generator><language>ja</language><copyright>© {year}</copyright><lastBuildDate>Fri, 02 Aug 2024 00:55:54 +0000</lastBuildDate><atom:link href="https://tsumli.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>CUDA: Cooperative Groupsについて</title><link>https://tsumli.github.io/blog/cuda/cooperative-groups/</link><pubDate>Fri, 02 Aug 2024 00:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/cuda/cooperative-groups/</guid><description>&lt;p>&lt;a href="https://developer.nvidia.com/blog/cooperative-groups/" 
 
 target="_blank" rel="noreferrer noopener" 
>NVIDIAのblog&lt;/a>
 を読んでいたのですが良く分からなかったのでコードを動かしながら見ていきます。&lt;/p>
&lt;h2 id="cooperative-groups-とは">Cooperative Groups とは?&lt;/h2>

 





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/overview.png"
 
 
 href="https://tsumli.github.io/blog/cuda/cooperative-groups/images/overview.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/cuda/cooperative-groups/images/overview.png"
 
 alt="Concepts of Cooperative Groups."
 
 
 />
 &lt;/a>
 
 
 &lt;figcaption>
 &lt;span class="img--caption">
 Figure . Concepts of Cooperative Groups.
 
 &lt;/span>
 &lt;/figcaption>
 
&lt;/figure>




&lt;p>今までは&lt;code>__syncthreads()&lt;/code>を使わないと同期できませんでした。しかし、これよりも小さなグループで同期を取りたいことがあります。Cooperative GroupsはCUDA 9から導入された機能で、これを使うことによって柔軟な同期が可能になります。&lt;/p></description></item><item><title>新しいPCを組んだので構成など</title><link>https://tsumli.github.io/blog/personal/desktop-20231201/</link><pubDate>Fri, 01 Dec 2023 00:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/personal/desktop-20231201/</guid><description>&lt;p>新しいPCを組んだので構成などのメモです (自分用) 。
セールなど使ってちょこちょこ買ったのですが計50万円は超えました&amp;hellip;&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>&lt;/th>
 &lt;th>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>CPU&lt;/td>
 &lt;td>intel Core i9-13900KF&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>CPUクーラー&lt;/td>
 &lt;td>MSI MEG CORELIQUID S360&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>GPU&lt;/td>
 &lt;td>MSI GeForce RTX 4090 GAMING X TRIO 24G&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>SSD&lt;/td>
 &lt;td>SAMSUNG 980 PRO 2TB&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>電源&lt;/td>
 &lt;td>Thermaltake TOUGHPOWER GF3 1200W&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>メモリ&lt;/td>
 &lt;td>Crucial CT2K32G4DFD832A x 2&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>マザーボード&lt;/td>
 &lt;td>MSI MAG Z790 TOMAHAWK WIFI&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>ケース&lt;/td>
 &lt;td>Fractal Design Define 7&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>OS&lt;/td>
 &lt;td>Ubuntu 22.04&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>グリス&lt;/td>
 &lt;td>ARCTIC MX-4&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>ケース&lt;/td>
 &lt;td>Fractal Design Define 7 Black Solid&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>ファン&lt;/td>
 &lt;td>Scythe KAZE FLEX 140 SQUARE PWM 1200rpm KF1425FD12S-P&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>ファン2&lt;/td>
 &lt;td>Noctua NF-A12X25 PWM&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>この構成で良かった所とダメだった所を考えてみます。&lt;/p></description></item><item><title>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation を読む</title><link>https://tsumli.github.io/blog/paper/bisenet/</link><pubDate>Tue, 06 Sep 2022 10:33:39 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/bisenet/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1808.00897" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation&lt;/strong>&lt;/a>
&lt;br>
[Yu et al. ECCV 2018] という論文を読んでいきます。 (本文中の図は論文より引用) 。
semantic segmentationの課題である、リアルタイム性 (推論速度) と精度のトレードオフ
を解決する新たなネットワークBilateral Segmentation Network (BiSeNet) の提案。&lt;/p></description></item><item><title>Lagrangian Fluid Simulation With Continuous Convolutions を読む</title><link>https://tsumli.github.io/blog/paper/deeplagrangianfluids/</link><pubDate>Fri, 08 Jul 2022 15:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/deeplagrangianfluids/</guid><description>&lt;p>&lt;a href="https://openreview.net/pdf?id=B1lDoJSYDH" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>Lagrangian Fluid Simulation
With Continuous Convolutions&lt;/strong>&lt;/a>
 という論文を読んでいきます (本文中の図は論文より引用) 。
この論文では液体をグラフとして扱う&amp;hellip;のではなくspatial convolutionを使って近傍の粒子との関係を計算します。&lt;/p></description></item><item><title>First Order Motion Model for Image Animation (FOMM) を読む</title><link>https://tsumli.github.io/blog/paper/fomm/</link><pubDate>Tue, 01 Feb 2022 19:58:16 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/fomm/</guid><description>&lt;p>&lt;a href="https://proceedings.neurips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>First Order Motion Model for Image Animation&lt;/strong>&lt;/a>
&lt;br>
[A. Siarohin et al. NeurIPS 2019] という論文を読んでいきます (本文中の図は論文より引用)。
FOMMという名前で知られている手法です。&lt;/p>

 





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/result.png"
 
 
 href="https://tsumli.github.io/blog/paper/fomm/images/result.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/paper/fomm/images/result.png"
 
 alt="example animation"
 
 
 />
 &lt;/a>
 
 
 &lt;figcaption>
 &lt;span class="img--caption">
 Figure . example animation
 
 &lt;/span>
 &lt;/figcaption>
 
&lt;/figure>




&lt;h2 id="method">Method&lt;/h2>
&lt;h3 id="目的">目的&lt;/h3>
&lt;p>&lt;strong>source image $\mathbf{S}$ と deiving video $\mathcal{D}$ の動きを表す潜在表現を組み合わせて、 driving video を再構成する&lt;/strong>&lt;/p></description></item><item><title>Video Swin Transformer を読む</title><link>https://tsumli.github.io/blog/paper/video-swin-transformer/</link><pubDate>Thu, 14 Oct 2021 15:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/video-swin-transformer/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2106.13230" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>Video Swin Transformer&lt;/strong>&lt;/a>
という論文を読んでいきます (本文中の図は論文より引用) 。ざっくり言うと、Swin Transformerをそのままvideoの入力に拡張した論文です。&lt;/p></description></item><item><title>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む</title><link>https://tsumli.github.io/blog/paper/swin-transformer/</link><pubDate>Thu, 14 Oct 2021 13:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/swin-transformer/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2103.14030" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/strong>&lt;/a>
という論文を読んでいきます (本文中の図は論文より引用) 。&lt;/p>





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/compare_vit.png"
 
 
 href="https://tsumli.github.io/blog/paper/swin-transformer/images/compare_vit.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/paper/swin-transformer/images/compare_vit.png"
 
 
 
 />
 &lt;/a>
 
 
&lt;/figure>




&lt;h2 id="method">Method&lt;/h2>
&lt;p>
 





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/architecture.png"
 
 
 href="https://tsumli.github.io/blog/paper/swin-transformer/images/architecture.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/paper/swin-transformer/images/architecture.png"
 
 alt="The architecture of a Swin Transformer (b) two successive Swin Transformer Blocks."
 
 
 />
 &lt;/a>
 
 
 &lt;figcaption>
 &lt;span class="img--caption">
 Figure 1. The architecture of a Swin Transformer (b) two successive Swin Transformer Blocks.
 
 &lt;/span>
 &lt;/figcaption>
 
&lt;/figure>




Swin Transformerの全体像 (小さいバージョン)。
まず、RGB画像をオーバーラップしないようにpatchに分ける。そして、そのRGB値をconcatしたものが特徴量として扱われる。
この論文では、$4\times 4$ のpatchに分けている。つまり、 特徴量は $4\times 4 \times 3 = 48$ 次元となる。
この特徴量は線形層に通され、任意の次元に投影される。&lt;/p></description></item><item><title>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing を読む</title><link>https://tsumli.github.io/blog/paper/one-shot-talking-head/</link><pubDate>Wed, 13 Oct 2021 13:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/one-shot-talking-head/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2011.15126" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing&lt;/strong>&lt;/a>
という論文を読んでいきます。CVPR 2021にacceptされています (本文中の図は論文より引用) 。&lt;/p>

 





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/overview.png"
 
 
 href="https://tsumli.github.io/blog/paper/one-shot-talking-head/images/overview.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/paper/one-shot-talking-head/images/overview.png"
 
 alt="提案手法の結果。H.264より10倍以上効率的にビデオを圧縮できる。また、画像のposeを自由に変更することができる。"
 
 
 />
 &lt;/a>
 
 
 &lt;figcaption>
 &lt;span class="img--caption">
 Figure . 提案手法の結果。H.264より10倍以上効率的にビデオを圧縮できる。また、画像のposeを自由に変更することができる。
 
 &lt;/span>
 &lt;/figcaption>
 
&lt;/figure>




&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Contrubutionは次の通り&lt;/p></description></item><item><title>TGAN: Synthesizing Tabular Data using Generative Adversarial Networks を読む</title><link>https://tsumli.github.io/blog/paper/TGAN/</link><pubDate>Mon, 14 Jun 2021 10:33:39 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/TGAN/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1811.11264" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>Synthesizing Tabular Data using Generative Adversarial Networks&lt;/strong>&lt;/a>
&lt;br>
[Xu et al. arXiv 2018] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p>
&lt;p>tabular dataに対するGAN (TGAN) についての紹介.
様々なデータ (categorical, numericalなど) が混合したようなテーブルに対して用いることができる.&lt;/p></description></item><item><title>FNet: Mixing Tokens with Fourier Transforms を読む</title><link>https://tsumli.github.io/blog/paper/FNet/</link><pubDate>Thu, 03 Jun 2021 07:11:44 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/FNet/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/2105.03824" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>FNet: Mixing Tokens with Fourier Transforms&lt;/strong>&lt;/a>
&lt;br>
[Thorp et al. arXiv 2020] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p>
&lt;p>Contributions:&lt;/p>
&lt;ol>
&lt;li>token &amp;ldquo;mixing&amp;rdquo; 変換がテキストデータにおける多様なsemanticsを十分に捉えられることを示した&lt;/li>
&lt;li>self-attention層をFourier Transform層に置き換えたTransformer-likeな &lt;strong>FNet&lt;/strong> というモデルの提案&lt;/li>
&lt;li>学習速度が早く (TPUでは短いシークエンスのときのみ), 精度も良い. また, メモリ使用量も比較的少なくすむ&lt;/li>
&lt;/ol>
&lt;h2 id="model">Model&lt;/h2>
&lt;h3 id="background-discrete-fourier-transforms">Background: discrete Fourier Transforms&lt;/h3>
&lt;p>シークエンス $\lbrace x_n \rbrace$ $\left( n \in [0, N-1] \right)$ が与えられたとき,
discrete Fourier Transform (DFT) は次のように表される.&lt;/p></description></item><item><title>GANimation を読む</title><link>https://tsumli.github.io/blog/paper/GANimation/</link><pubDate>Sat, 22 May 2021 11:38:22 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/GANimation/</guid><description>&lt;!-- define macro -->
&lt;p>$$
\def\image#1{\mathbf{I}_{\mathbf{y}{\scriptsize{#1}}}}
$$&lt;/p>
&lt;!-- end define -->
&lt;p>&lt;a href="https://arxiv.org/abs/1807.09251" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>GANimation: Anatomically-aware Facial Animation from a Single Image&lt;/strong>&lt;/a>
&lt;br>
[Pumarola et al. ECCV 2018] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p>
&lt;p>Action Units (AU) アノテーションに基づいたGAN

 





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/synthesis_result.png"
 
 
 href="https://tsumli.github.io/blog/paper/GANimation/images/synthesis_result.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/paper/GANimation/images/synthesis_result.png"
 
 alt="Facial animation from a single image."
 
 
 />
 &lt;/a>
 
 
 &lt;figcaption>
 &lt;span class="img--caption">
 Figure . Facial animation from a single image.
 
 &lt;/span>
 &lt;/figcaption>
 
&lt;/figure>



&lt;/p></description></item><item><title>StarGAN v2 を読む</title><link>https://tsumli.github.io/blog/paper/StarGANv2/</link><pubDate>Fri, 21 May 2021 19:58:16 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/StarGANv2/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1912.01865" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>StarGAN v2: Diverse Image Synthesis for Multiple Domains&lt;/strong>&lt;/a>
&lt;br>
[Choi et al. CVPR 2020] という論文を読んでいきます. (本文中の図は論文より引用).&lt;/p>

 





&lt;figure
 
 
 
 style='margin:0 auto;text-align:center;'
 >
 &lt;a 
 
 data-lightbox="image-images/Synth_result.png"
 
 
 href="https://tsumli.github.io/blog/paper/StarGANv2/images/Synth_result.png"
 
 
 >
 &lt;img
 
 src="https://tsumli.github.io/blog/paper/StarGANv2/images/Synth_result.png"
 
 alt="Synthesis Result"
 
 
 />
 &lt;/a>
 
 
 &lt;figcaption>
 &lt;span class="img--caption">
 Figure . Synthesis Result
 
 &lt;/span>
 &lt;/figcaption>
 
&lt;/figure>




&lt;p>image-to-image translationのタスクにおいて, 異なるドメイン間のマッピングを学習しなければならない.
そして, それらは次の要素を満たす必要がある.&lt;/p></description></item><item><title>DockerでSingularityのImageをbuildする</title><link>https://tsumli.github.io/blog/abci/docker-build-singularity/</link><pubDate>Fri, 16 Apr 2021 18:01:52 +0000</pubDate><guid>https://tsumli.github.io/blog/abci/docker-build-singularity/</guid><description>&lt;p>dockerでsingularityのimage file (.sif) を作成したときのメモ書きです．&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>ローカルPCでsingularity buildしたい (docker image to singularity image) けど，ローカルPCの環境を汚したくない，という状況です．&lt;/p></description></item><item><title>Optimal Transport 03. Unbalanced Optimal Transport</title><link>https://tsumli.github.io/blog/optimal-transport/03-unbalanced/</link><pubDate>Thu, 11 Mar 2021 13:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/optimal-transport/03-unbalanced/</guid><description>&lt;p>&lt;a href="https://tsumli.github.io/blog/optimal-transport/01-introduction/" 
 
 
>01. はじめ&lt;/a>
&lt;br>
&lt;a href="https://tsumli.github.io/blog/optimal-transport/02-entropy/" 
 
 
>02. エントロピー正則化&lt;/a>
&lt;br>
&lt;a href="https://tsumli.github.io/blog/optimal-transport/03-unbalanced/" 
 
 
>03. Unbalanced Optimal Transport&lt;/a>
&lt;br>
Optimal Transportの拡張系である&lt;strong>Unbalanced Optimal Transport&lt;/strong>について紹介します．&lt;/p>
&lt;h2 id="unbalanced-optimal-transport-unbalanced-ot">Unbalanced Optimal Transport (Unbalanced OT)&lt;/h2>
&lt;p>まず，式を見ていきましょう．
$$
L_\mathbf{C}^\tau (\mathbf{a}, \mathbf{b}) = \min_{\mathbf{P}\in\mathbb{R}&lt;em>+^{n\times m}} \langle \mathbf{C}, \mathbf{P} \rangle
+\tau_1\mathbf{D}&lt;/em>\varphi(\mathbf{P}\mathbb{1}&lt;em>m|\mathbf{a}) +
\tau_2\mathbf{D}&lt;/em>\varphi(\mathbf{P}^\mathsf{T}\mathbb{1}&lt;em>n|\mathbf{b})
$$
第1項は通常のoptimal transportでの最小化目標です．そして，この第2, 3項が追加されたものとなります．$\mathbf{D}&lt;/em>\varphi(\cdot|\cdot)$ はズレを表す関数で，例えば，euclideanだったりKL距離だったりが考えられます．そして，$\tau_1, \tau_2$はそれらのズレに対するペナルティの大きさを表しています．つまり，$\tau_1 = \tau_2 \rightarrow +\infty$のときズレを許さない，つまり，&amp;ldquo;balanced&amp;quot;な (通常の) optimal transportとなります．また，$\mathbf{D}_\varphi = \mathbf{KL}$ で，$\tau_1 = \tau_2 \rightarrow 0$ のとき，Hellinger距離と呼ばれる距離となります．
$$
\mathrm{L}(\mathbf{a}, \mathbf{b}) = \sum_i(\sqrt{\mathbf{a}_i} - \sqrt{\mathbf{b}_i})^2
$$&lt;/p></description></item><item><title>Optimal Transport 02. エントロピー正則化</title><link>https://tsumli.github.io/blog/optimal-transport/02-entropy/</link><pubDate>Sat, 13 Feb 2021 13:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/optimal-transport/02-entropy/</guid><description>&lt;p>&lt;a href="https://tsumli.github.io/blog/optimal-transport/01-introduction/" 
 
 
>01. はじめ&lt;/a>
&lt;br>
&lt;a href="https://tsumli.github.io/blog/optimal-transport/02-entropy/" 
 
 
>02. エントロピー正則化&lt;/a>
&lt;br>
&lt;a href="https://tsumli.github.io/blog/optimal-transport/03-unbalanced/" 
 
 
>03. Unbalanced Optimal Transport&lt;/a>
&lt;/p>
&lt;p>Cuturi&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> の論文をもとに、Optimal Transportのエントロピー正則化について考えていきましょう 。&lt;/p>
&lt;h2 id="計算量">計算量&lt;/h2>
&lt;p>$r$ から $c$ へのOptimal Transportは次のようになります
$$
\large
d_\mathbf{M}(r, c) \triangleq \min_{\mathbf{P}\in\mathbf{U}(r, c)} \langle \mathbf{P}, \mathbf{M} \rangle
$$
ここで、$\mathbf{M} \in \mathbb{R}^{d\times d}$ はcost matrixです&lt;br>
計算量について考えると、どのような行列$\mathbf{M}$に対しても最悪のケースで $O(d^3 \log d)$となります。&lt;/p></description></item><item><title>Optimal Transport 01. はじめ</title><link>https://tsumli.github.io/blog/optimal-transport/01-introduction/</link><pubDate>Tue, 02 Feb 2021 13:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/optimal-transport/01-introduction/</guid><description>&lt;p>&lt;a href="https://tsumli.github.io/blog/optimal-transport/01-introduction/" 
 
 
>01. はじめ&lt;/a>
&lt;br>
&lt;a href="https://tsumli.github.io/blog/optimal-transport/02-entropy/" 
 
 
>02. エントロピー正則化&lt;/a>
&lt;br>
&lt;a href="https://tsumli.github.io/blog/optimal-transport/03-unbalanced/" 
 
 
>03. Unbalanced Optimal Transport&lt;/a>
&lt;/p>
&lt;h2 id="optimal-transport-とは">Optimal Transport とは&lt;/h2>
&lt;p>&lt;strong>Optimal Transport&lt;/strong>は、日本語で&lt;strong>最適輸送&lt;/strong> (問題) と訳されます。
和訳が表しているように、ある物体をある地点から別の地点に移したときの最小コスト、そしてその輸送方法を求めるという問題です。そこから、確率分布の比較に使われるようになりました。&lt;/p></description></item><item><title>自分的Singularityの使い方</title><link>https://tsumli.github.io/blog/abci/singularity/</link><pubDate>Mon, 01 Feb 2021 15:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/abci/singularity/</guid><description>&lt;h2 id="why-singularity">Why Singularity?&lt;/h2>
&lt;p>abciを使うときにdockerではなくsingularityを使う必要があったのですが、戸惑う部分が多かったので記録しておきます。
dockerはroot権限が奪取される可能性があるため、共用サーバなどではセキュリティの問題から使用できない場面が多いです。&lt;/p></description></item><item><title>Taskonomyを読む</title><link>https://tsumli.github.io/blog/paper/taskonomy/</link><pubDate>Mon, 01 Feb 2021 13:55:54 +0000</pubDate><guid>https://tsumli.github.io/blog/paper/taskonomy/</guid><description>&lt;p>&lt;a href="https://arxiv.org/abs/1804.08328" 
 
 target="_blank" rel="noreferrer noopener" 
>&lt;strong>Taskonomy: Disentangling Task Transfer Learning&lt;/strong>&lt;/a>
という論文を読んでいきます。
この論文はCVPR 2018のBestPaperを受賞しています (本文中の図は論文より引用) 。&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>タスク間の転移学習しやすさが分かれば、アノテーションの足りないデータを扱う、または性能を向上させたいときにどのタスクで事前学習を行うべきかが分かる。&lt;/p></description></item></channel></rss>