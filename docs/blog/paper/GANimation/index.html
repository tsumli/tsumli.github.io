<!doctype html><html lang=ja><head><title>GANimation を読む | tsumli-pages</title><meta charset=UTF-8><meta name=language content="en"><meta name=description content><meta name=keywords content="GANimation ,GAN"><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel="shortcut icon" type=image/png href=https://tsumli.github.io/favicon.ico><link type=text/css rel=stylesheet href=https://tsumli.github.io/css/post.min.56c8810b0f07bc4b6fe3dd802a8194d08a2ed5a4903bdf2a4859c17f3860a7e7.css integrity="sha256-VsiBCw8HvEtv492AKoGU0Iou1aSQO98qSFnBfzhgp+c="><link type=text/css rel=stylesheet href=https://tsumli.github.io/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/tsumli.github.io\/"},"articleSection":"blog","name":"GANimation を読む","headline":"GANimation を読む","description":"","inLanguage":"en-US","author":"","creator":"","publisher":"","accountablePerson":"","copyrightHolder":"","copyrightYear":"2021","datePublished":"2021-05-22 11:38:22 \u002b0000 UTC","dateModified":"2021-05-22 11:38:22 \u002b0000 UTC","url":"https:\/\/tsumli.github.io\/blog\/paper\/GANimation\/","wordCount":"1643","keywords":["GANimation","GAN","Blog"]}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=https://tsumli.github.io/>about</a></li><li><a class=active href=https://tsumli.github.io/blog>blog</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>GANimation を読む</h1><time datetime="2021-05-22 11:38:22 +0000 UTC" class=post__date>May 22 2021</time>
<link rel=stylesheet href=https://tsumli.github.io//css/lightbox.min.css><script type=text/javascript>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,tags:"ams",autoload:{color:[],colorV2:["color"]},packages:{"[+]":["noerrors"]}},chtml:{matchFontHeight:!1,displayAlign:"left",displayIndent:"2em"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],renderActions:{find_script_mathtex:[10,function(e){for(const t of document.querySelectorAll('script[type^="math/tex"]')){const o=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],o),s=document.createTextNode("");t.parentNode.replaceChild(s,t),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},e.math.push(n)}},""]}},loader:{load:["[tex]/noerrors"]}}</script><script type=text/javascript async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js id=MathJax-script></script><link rel=stylesheet type=text/css href=https://tsumli.github.io/css/mathjax-style.css></header><article class=post__content><p>$$
\def\image#1{\mathbf{I}_{\mathbf{y}{\scriptsize{#1}}}}
$$</p><p><a href=https://arxiv.org/abs/1807.09251 target=_blank rel="noreferrer noopener"><strong>GANimation: Anatomically-aware Facial Animation from a Single Image</strong></a><br>[Pumarola et al. ECCV 2018] という論文を読んでいきます. (本文中の図は論文より引用).</p><p>Action Units (AU) アノテーションに基づいたGAN<figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/synthesis_result.png href=https://tsumli.github.io/blog/paper/GANimation/images/synthesis_result.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/synthesis_result.png alt="Facial animation from a single image."></a><figcaption><span class=img--caption>Figure . Facial animation from a single image.</span></figcaption></figure></p><p>$\image{r}$ (緑色で囲われた画像) は入力画像.
そして, $\alpha$ はsmiling-like expressionに関するAUをコントロールするパラメータ</p><h2 id=introduction>Introduction<a class=anchor href=#introduction>#</a></h2><p>motivation: single imageからfacial expressionを操作する $\rightarrow$ 映画業界, 写真技術, ファッションやe-コマースに役立つ</p><p>facial expression は, discreteでも少ないクラスでも表されない表情筋によって作られるものである.
Facial Action Coding System (FACS) [Ekman et al.] は 解剖学的に特定の表情筋の収縮に関わるAction Units (AUs) を表すために提案されたものである.
AUの数は少ないが, $7000$もの異なる組み合わせがあることがわかっている.</p><p>同じ人の異なる表情が必要になるのを避けるために, 問題を2つに分ける</p><ol><li>AU-conditioned bidirectional adversarial architecture<br>1つの訓練画像が与えられた時, まず, 欲しい表情の新しい画像を作る.
そして, この作られた画像を生成画像のphotorealismに対するロスを取り入れる.</li><li>画像の中で, 新しい表情の伝達に関わる部分に対してフォーカスするattention layerを用いることでSOTAを達成.</li></ol><h2 id=problems-formulation>Problems Formulation<a class=anchor href=#problems-formulation>#</a></h2><p>任意のfacial expressionのRGB画像は次のように表される: $\image{r} \in \mathbb{R}^{H\times W \times 3}$<br>また, すべてのgesture expressionは $N$ 個のAUにエンコードされる $\mathbf{y}_r = (y_1, \ldots, y_N)^\mathsf{T}$
(それぞれの要素は$0&ndash;1$に正規化される). 連続的なラベルなので, 異なる表情間の自然なinterpolationが可能となる.</p><p>目的は, 画像間のマッピングを行う $\mathcal{M}$ を学習すること.
\begin{align*}
\mathcal{M}: (\image{r}, \mathbf{y}_g) \rightarrow \image{g} \quad (\text{conditioned on AU target }\mathbf{y}_g)
\end{align*}
つまり, $\image{r}$ を AU label $\mathbf{y}_g$ をもつ画像に変換する.</p><p>この学習は $M$個のtriplets:
\begin{align*}
\lbrace \image{r}^m, \mathbf{y}_r^m, \mathbf{y}_g^m \rbrace \quad (m=1, \ldots, M)
\end{align*}
を用いてunsupervisedに行われる. ここで, $\mathbf{y}_g^m$ はランダムに生成されたtarget vector.
そして, この手法では, 同じ人の異なる表情や expected target image $\image{g}$ を必要としない.</p><h2 id=approach>Approach<a class=anchor href=#approach>#</a></h2><p><figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/overview.png href=https://tsumli.github.io/blog/paper/GANimation/images/overview.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/overview.png alt="Overview of the proposed method."></a><figcaption><span class=img--caption>Figure 1. Overview of the proposed method.</span></figcaption></figure>提案手法は2つのモジュールからなる.</p><p><strong>Generator</strong>: $G(\image{r} | \mathbf{y}_g)$<br>このgeneratorは2回使われる</p><ol><li>$\image{r} \rightarrow \image{g}$ (map the input image)</li><li>$\image{g} \rightarrow \hat{\mathbf{I}}_{\mathbf{y}{\scriptsize{r}}}$ (render back)</li></ol><p><strong>Discriminator</strong>: $D(\image{g})$<br>WGAN-GPを用いた, 生成画像のqualityと表情を評価するもの</p><h3 id=network-architecture>Network Architecture<a class=anchor href=#network-architecture>#</a></h3><h4 id=generator>Generator</h4><p>画像 $\image{o} \in \mathbb{R}^{H\times W \times 3}$ と $N$次元ベクトルにエンコードされた感情 (desired expression) $\mathbf{y}_f$ が与えられたとき,
generatorに対する入力は次のようになる
$$
(\image{o}, \mathbf{y}_o) \in \mathbb{R}^{H\times W \times (N+3)}
$$
($\mathbf{y}_o$は$\mathbf{y}_f$のブロードキャストしたもの <a href=https://github.com/albertpumarola/GANimation/blob/40da9ae1a88e6a3c38cd6548be08a61cbd6695ba/networks/generator_wasserstein_gan.py#L48-L54 target=_blank rel="noreferrer noopener">Code</a>
)</p><p>下図にgeneratorのアーキテクチャを示す<figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/gen.png href=https://tsumli.github.io/blog/paper/GANimation/images/gen.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/gen.png alt="Attention-based generator."></a><figcaption><span class=img--caption>Figure 2. Attention-based generator.</span></figcaption></figure>generatorは2つのmask (color mask $\mathbf{C}$ と attention mask $\mathbf{A}$) を生成する.
そして, 最終的な画像は<a href=https://github.com/albertpumarola/GANimation/blob/40da9ae1a88e6a3c38cd6548be08a61cbd6695ba/models/ganimation.py#L133 target=_blank rel="noreferrer noopener">次のように得られる</a>
$$
\image{f} = (1-\mathbf{A}) \cdot \mathbf{C} + \mathbf{A} \cdot \image{o}
$$
ここで, $\mathbf{A} = G_A(\image{o}| \mathbf{y}_f) \in \lbrace0, \ldots, 1\rbrace^{H\times W}$, $\mathbf{C} = G_C(\image{o}|\mathbf
{y}_f) \in \mathbb{R}^{H\times W\times 3}$<br>$\mathbf{A}$ は $\mathbf{C}$ が生成した画像のそれぞれのピクセルに対する重要性を表す.</p><h4 id=conditional-critic>Conditional Critic</h4><p>$D$ のアーキテクチャは<a href=https://arxiv.org/abs/1611.07004 target=_blank rel="noreferrer noopener">PatchGAN</a>
に似たものである.<br>PatchGANでは, 入力画像を $\mathbf{I}$ とすると, $\mathbf{Y}_\mathbf{I} \in \mathbb{R}^{H/2^6 \times W / 2^6}$を生成する.</p><p>ここで, $\mathbf{Y}_\mathbf{I} [i, j]$ は patch $ij$ が (生成されたフェイク画像ではなく) 真の画像であるかを表す確率である.
その最終層にregression headを追加し, 画像のAUs activation $\hat{\mathbf{y}} = (\hat{y}_1, \ldots, \hat{y}_N)^\mathsf{T}$ を推測する</p><h3 id=learning-the-model>Learning the Model<a class=anchor href=#learning-the-model>#</a></h3><p>ロス関数は4つの項からなる. (細かな定義などは論文参照)</p><h4 id=1-image-adversarial-loss-mathcall_mathbfi>1. Image Adversarial Loss $\mathcal{L}_\mathbf{I}$</h4><p>Earth Mover Distanceを用いたWGANを参考にする.</p><figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/loss_1.png href=https://tsumli.github.io/blog/paper/GANimation/images/loss_1.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/loss_1.png></a></figure><h4 id=2-attention-loss-mathcall_mathbfa>2. Attention Loss $\mathcal{L}_\mathbf{A}$</h4><p>attention maskが1になる (サチる) ことを防ぐためのロス<figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/loss_2.png href=https://tsumli.github.io/blog/paper/GANimation/images/loss_2.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/loss_2.png></a></figure></p><h4 id=3-conditional-expression-loss-mathcall_y>3. Conditional Expression Loss $\mathcal{L}_y$</h4><p>AUのregression headに対するロス<figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/loss_3.png href=https://tsumli.github.io/blog/paper/GANimation/images/loss_3.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/loss_3.png></a></figure></p><h4 id=4-identity-loss-mathcall_idt>4. Identity Loss $\mathcal{L}_{idt}$</h4><p>inputとoutputが同じ人を表すようにするためのロス<figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/loss_4.png href=https://tsumli.github.io/blog/paper/GANimation/images/loss_4.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/loss_4.png></a></figure></p><h4 id=full-loss>Full Loss</h4><figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/loss_all.png href=https://tsumli.github.io/blog/paper/GANimation/images/loss_all.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/loss_all.png></a></figure><p>そして最終的に, 次のminimax問題を解く.
$$
G^* = \arg \min_{G} \max_{D} \mathcal{L}
$$</p><h2 id=experimental-evaluation>Experimental Evaluation<a class=anchor href=#experimental-evaluation>#</a></h2><p><figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/res_comparison.png href=https://tsumli.github.io/blog/paper/GANimation/images/res_comparison.png><img src=https://tsumli.github.io/blog/paper/GANimation/images/res_comparison.png alt="Qualitative comparison with state-of-the-art"></a><figcaption><span class=img--caption>Figure 8. Qualitative comparison with state-of-the-art</span></figcaption></figure>提案手法がvisual accuracyとspatial resolutionのトレードオフをうまく達成していることがわかる (例えば, StarGANの結果はブラーがかかっている).
その他結果は元論文で&mldr;</p><h2 id=references>References<a class=anchor href=#references>#</a></h2><ul><li>GANimation<br><a href=https://arxiv.org/abs/1807.09251 target=_blank rel="noreferrer noopener">arXiv</a><br><a href=https://www.albertpumarola.com/research/GANimation/index.html target=_blank rel="noreferrer noopener">project page</a></li></ul><p><a href=https://github.com/albertpumarola/GANimation target=_blank rel="noreferrer noopener"><img src=https://gh-card.dev/repos/albertpumarola/GANimation.svg alt="albertpumarola/GANimation - GitHub"></a></p></article><ul class=tags__list><li class=tag__item><a class=tag__link href=https://tsumli.github.io/tags/gan/>GAN</a></li><li class=tag__item><a class=tag__link href=https://tsumli.github.io/tags/paper/>paper</a></li></ul><div class=pagination><a class=pagination__item href=https://tsumli.github.io/blog/paper/StarGANv2/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>StarGAN v2 を読む</span>
</a><a class=pagination__item href=https://tsumli.github.io/blog/paper/FNet/><span class=pagination__label>Next Post</span>
<span class=pagination__title>FNet: Mixing Tokens with Fourier Transforms を読む</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link rel=me title=Kaggle href=https://www.kaggle.com/tsumli target=_blank rel=noopener><div class=social-icons__icon style=background-image:url(https://tsumli.github.io/svg/kaggle.svg)></div></a><a class=social-icons__link rel=me title=GitHub href=https://github.com/tsumli target=_blank rel=noopener><div class=social-icons__icon style=background-image:url(https://tsumli.github.io/svg/github.svg)></div></a></div><p>© 2025</p><script src=https://code.jquery.com/jquery-3.4.1.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><link rel=stylesheet href=https://tsumli.github.io//css/lightbox.min.css><script src=https://tsumli.github.io//js/lightbox.min.js></script></footer></div></div><div class=toc-container><div class=toc-post-title>GANimation を読む</div><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#problems-formulation>Problems Formulation</a></li><li><a href=#approach>Approach</a><ul><li><a href=#network-architecture>Network Architecture</a></li><li><a href=#learning-the-model>Learning the Model</a></li></ul></li><li><a href=#experimental-evaluation>Experimental Evaluation</a></li><li><a href=#references>References</a></li></ul></nav></div></div></main><script src=https://tsumli.github.io/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin=anonymous></script><script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script><script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script><script src=https://tsumli.github.io/js/table-of-contents.js></script></body></html>