<!doctype html><html lang=ja><head><title>StarGAN v2 を読む | tsumli-pages</title><meta charset=utf-8><meta name=language content="en"><meta name=description content><meta name=keywords content="StarGAN ,GAN"><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><link rel="shortcut icon" type=image/png href=https://tsumli.github.io/favicon.ico><link type=text/css rel=stylesheet href=https://tsumli.github.io/css/post.min.86d1effd4c412b85ac13db53a90c473a0f256f789b821e131125c9aa25cb6a6d.css integrity="sha256-htHv/UxBK4WsE9tTqQxHOg8lb3ibgh4TESXJqiXLam0="><link type=text/css rel=stylesheet href=https://tsumli.github.io/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/tsumli.github.io\/"},"articleSection":"blog","name":"StarGAN v2 を読む","headline":"StarGAN v2 を読む","description":"","inLanguage":"en-US","author":"","creator":"","publisher":"","accountablePerson":"","copyrightHolder":"","copyrightYear":"2021","datePublished":"2021-05-21 19:58:16 \u002b0000 UTC","dateModified":"2021-05-21 19:58:16 \u002b0000 UTC","url":"https:\/\/tsumli.github.io\/blog\/paper\/StarGANv2\/","wordCount":"1265","keywords":["StarGAN","GAN","Blog"]}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-188554402-1','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=https://tsumli.github.io/>about</a></li><li><a class=active href=https://tsumli.github.io/blog>blog</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>StarGAN v2 を読む</h1><time datetime="2021-05-21 19:58:16 +0000 UTC" class=post__date>May 21 2021</time>
<link rel=stylesheet href=https://tsumli.github.io//css/lightbox.min.css><script type=text/javascript>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],processEscapes:!0,tags:"ams",autoload:{color:[],colorV2:['color']},packages:{'[+]':['noerrors']}},chtml:{matchFontHeight:!1,displayAlign:"left",displayIndent:"2em"},options:{skipHtmlTags:['script','noscript','style','textarea','pre'],renderActions:{find_script_mathtex:[10,function(a){for(const b of document.querySelectorAll('script[type^="math/tex"]')){const e=!!b.type.match(/; *mode=display/),c=new a.options.MathItem(b.textContent,a.inputJax[0],e),d=document.createTextNode('');b.parentNode.replaceChild(d,b),c.start={node:d,delim:'',n:0},c.end={node:d,delim:'',n:0},a.math.push(c)}},'']}},loader:{load:['[tex]/noerrors']}}</script><script type=text/javascript async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js id=MathJax-script></script></header><article class=post__content><p><a href=https://arxiv.org/abs/1912.01865 target=_blank rel="noreferrer noopener"><strong>StarGAN v2: Diverse Image Synthesis for Multiple Domains</strong></a><br>[Choi et al. CVPR 2020] という論文を読んでいきます. (本文中の図は論文より引用).</p><figure style="margin:0 auto;text-align:center"><a data-lightbox=image-images/Synth_result.png href=https://tsumli.github.io/blog/paper/StarGANv2/images/Synth_result.png><img src=https://tsumli.github.io/blog/paper/StarGANv2/images/Synth_result.png alt="Synthesis Result"></a><figcaption><span class=img--caption>Figure . Synthesis Result</span></figcaption></figure><p>image-to-image translationのタスクにおいて, 異なるドメイン間のマッピングを学習しなければいけません.
そして, それらは次の要素を満たす必要があります.</p><ol><li>生成した画像のdiversityが広い</li><li>様々にドメインに対応できる (scalability)</li></ol><h2 id=introduction>introduction<a class=anchor href=#introduction>#</a></h2><p>既存手法では, マッピングが2つのドメイン間に限定される (scalabilityがない) という問題点があったが,
<a href=https://arxiv.org/abs/1711.09020 target=_blank rel="noreferrer noopener"><strong>StarGAN</strong></a>
は1つのgeneratorで複数のドメインに対応するモデルを提案した.
この手法では, generatorは追加のinputとしてドメインのラベルを持ち, 画像を目的のドメインに変換させる. しかし,
StarGANはそれぞれのドメイン間のマッピングを学習しているだけで, データ分布のマルチモーダルな特徴を捉えているわけではない.</p><h2 id=method>Method<a class=anchor href=#method>#</a></h2><h3 id=framework>Framework<a class=anchor href=#framework>#</a></h3><p><figure style="margin:0 auto;text-align:center"><a data-lightbox=image-images/overview.png href=https://tsumli.github.io/blog/paper/StarGANv2/images/overview.png><img src=https://tsumli.github.io/blog/paper/StarGANv2/images/overview.png alt=Overview></a><figcaption><span class=img--caption>Figure 1. Overview</span></figcaption></figure>ゴールは <strong>様々な</strong>画像を生成できる<strong>1つ</strong>のgenerator $G$ を訓練すること.</p><h4 id=generator-figure-1a>Generator (Figure 1a)</h4><p>$\mathbf x \in \mathcal{X}$ を画像, $y \in \mathcal{Y}$ をドメインとする.
$G$ は 画像とstyle code $\mathbf s$ を引数にとり, 新たな画像を生成する (AdaINを用いて $\mathbf s$ を $G$ に与える).
$y$ を $G$ に与えないことで, すべてのドメインに対応する画像が生成できる.</p><h4 id=mapping-network-figure-1b>Mapping network (Figure 1b)</h4><p>latent code $\mathbf z$ とドメイン $y$が与えられたとき,
mapping network $F$ は style code $\mathbf{s}$ を生成する
$$
\mathbf s = F_y (\mathbf z)
$$</p><h4 id=style-encoder-figure-1c>Style encoder (Figure 1c)</h4><p>画像 $\mathbf{x}$ と対応するドメイン $y$ が与えられたとき,
encoder $E$ は $\mathbf x$ に対応するstyle code $\mathbf s$ を生成する
$$
\mathbf s = E_y(\mathbf x)
$$</p><h4 id=discriminator-figure-1d>Discriminator (Figure 1d)</h4><p>discriminator $D$ は複数のoutput branchを持つmulti-task discriminatorである.
それぞれのbranch $D_y$ は$\mathbf x$ が $y$ に属する本当の画像か, それとも偽物の画像か判定する.</p><h3 id=training-objectives>Training Objectives<a class=anchor href=#training-objectives>#</a></h3><p>4つのロスを組み合わせたロスを用いる</p><h4 id=1-adversarial-objective>1. Adversarial objective</h4><p>訓練するとき, latent code $\mathbf z$ とターゲットドメイン $\tilde y \in \mathcal{Y}$ をランダムにとってくる.
そして, ターゲットstyle code $\tilde{\mathbf s} = F_\tilde{y}(\mathbf{z})$ を求めたあと,
$\mathbf{x}$ と $\tilde{\mathbf{s}}$ を入力として $G$ を学習させる.
このとき, adversarial lossを用いる.
$$
\mathcal{L}_{adv} = \mathbb{E}_{\mathbf x, y}[\log D_y(\mathbf x)] + \mathbb{E}_{\mathbf x, \tilde{y}, \mathbf{z}}[\log (1-D_\tilde{y}(G(\mathbf{x}, \tilde{\mathbf{s}})))]
$$</p><h4 id=2-style-reconstruction>2. Style reconstruction</h4><p>生成画像からstyle codeをマッピングし, それとstyle codeの間のロスを計算する</p><p>\begin{align*}
\mathcal{L}_{sty} &= \mathbb{E}_{\mathbf{x}, \tilde{y}, \mathbf{z}} [||\tilde{\mathbf{s}} - E_{\tilde{y}}(G(\mathbf{x}, \tilde{\mathbf{s}}))||_1]\\\<br>\end{align*}</p><h4 id=3-style-diversification>3. Style diversification</h4><p>$G$ が多様な画像を生成するためのロス
\begin{align*}
\mathcal{L}_{ds} &= \mathbb{E}_{\mathbf{x}, y, \mathbf{z}_1, \mathbf{z}_2} [||G(\mathbf{x}, \tilde{\mathbf{s}}_1) - G(\mathbf{x}, \tilde{\mathbf{s}}_2)||_1]\\\<br>\end{align*}
where
\begin{align*}
\tilde{\mathbf{s}}_i &= F_{\tilde{y}} (\mathbf{z}_i) & \mathrm{for}\ \ i \in {1, 2}\\\<br>\end{align*}</p><h4 id=4-preserving-source-characteristics>4. Preserving source characteristics</h4><p>$G(\mathbf{x}, \tilde{\mathbf{s}})$ が $\mathbf{x}$ におけるドメインによらない特徴 (ポーズなど) をとらえるためのロス
\begin{align*}
\mathcal{L}_{cyc} = \mathbb{E}_{\mathbf{x}, y, \tilde{y} \mathbf{z}} [||\mathbf{x} - G(G(\mathbf{x}, \tilde{\mathbf{s}}), \hat{\mathbf{s}})||_1]
\end{align*}
where
\begin{align*}
\hat{\mathbf{s}} &= E_y(\mathbf{x}) &\text{(入力画像}\ \mathbf{x}\ \text{に対するstyle codeの予測値)}
\end{align*}</p><h4 id=full-objective>Full objective</h4><p>4つのロスを組み合わせる
\begin{align*}
\min_{G, F, E} \max_D \ \ \mathcal{L}_{adv} + \lambda_{sty} \mathcal{L}_{sty} - \lambda_{ds} \mathcal{L}_{ds} + \lambda_{cyc} \mathcal{L}_{cyc}
\end{align*}</p><p>$\lambda_\cdot$: hyper-parameter</p><h2 id=experiments>Experiments<a class=anchor href=#experiments>#</a></h2><h3 id=baselines>Baselines<a class=anchor href=#baselines>#</a></h3><ul><li><a href=https://arxiv.org/abs/1804.04732 target=_blank rel="noreferrer noopener">MUNIT</a></li><li><a href=https://arxiv.org/abs/1808.00948 target=_blank rel="noreferrer noopener">DRIT</a></li><li><a href=https://arxiv.org/abs/1903.05628 target=_blank rel="noreferrer noopener">MSGAN</a></li><li><a href=https://arxiv.org/abs/1912.01865 target=_blank rel="noreferrer noopener">StarGAN</a></li></ul><h3 id=datasets>Datasets<a class=anchor href=#datasets>#</a></h3><ul><li><a href=https://arxiv.org/abs/1710.10196 target=_blank rel="noreferrer noopener">CelebA-HQ</a></li><li>AFHQ</li></ul><p>画像は 256 $\times$ 256 にリサイズされる</p><h3 id=evaluation-metrics>Evaluation metrics<a class=anchor href=#evaluation-metrics>#</a></h3><p>qualityとdiversityを評価するために, FIDとLPIPSを用いる</p><h2 id=results>Results<a class=anchor href=#results>#</a></h2><figure style="margin:0 auto;text-align:center"><a data-lightbox=image-images/Ref-guided-res.png href=https://tsumli.github.io/blog/paper/StarGANv2/images/Ref-guided-res.png><img src=https://tsumli.github.io/blog/paper/StarGANv2/images/Ref-guided-res.png alt="Reference-guided image synthesis results on CelebA-HQ"></a><figcaption><span class=img--caption>Figure 2. Reference-guided image synthesis results on CelebA-HQ</span></figcaption></figure><p>StarGAN v2ではdiverseな画像を生成することがわかる (reference画像の髪型やメイク, 髭などの特徴をsource画像の特徴を崩すことなく生成している).</p><p>その他結果は論文で&mldr;</p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script></article><ul class=tags__list><li class=tag__item><a class=tag__link href=https://tsumli.github.io/tags/gan/>GAN</a></li><li class=tag__item><a class=tag__link href=https://tsumli.github.io/tags/paper/>paper</a></li></ul><div class=pagination><a class=pagination__item href=https://tsumli.github.io/blog/abci/docker-build-singularity/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>DockerでSingularityのImageをbuildする</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link rel=me title=GitHub href=https://github.com/tsumli target=_blank rel=noopener><div class=social-icons__icon style=background-image:url(https://tsumli.github.io/svg/github.svg)></div></a><a class=social-icons__link rel=me title=Medium href=https://medium.com/@tsumli target=_blank rel=noopener><div class=social-icons__icon style=background-image:url(https://tsumli.github.io/svg/medium.svg)></div></a></div><p>© 2021</p><script src=https://code.jquery.com/jquery-3.4.1.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><link rel=stylesheet href=https://tsumli.github.io//css/lightbox.min.css><script src=https://tsumli.github.io//js/lightbox.min.js></script></footer></div></div><div class=toc-container><div class=toc-post-title>StarGAN v2 を読む</div><nav id=TableOfContents><ul><li><a href=#introduction>introduction</a></li><li><a href=#method>Method</a><ul><li><a href=#framework>Framework</a></li><li><a href=#training-objectives>Training Objectives</a></li></ul></li><li><a href=#experiments>Experiments</a><ul><li><a href=#baselines>Baselines</a></li><li><a href=#datasets>Datasets</a></li><li><a href=#evaluation-metrics>Evaluation metrics</a></li></ul></li><li><a href=#results>Results</a></li></ul></nav></div></div></main><script src=https://tsumli.github.io/js/index.min.575dda8d49ee02639942c63564273e6da972ab531dda26a08800bdcb477cbd7f.js integrity="sha256-V13ajUnuAmOZQsY1ZCc+balyq1Md2iagiAC9y0d8vX8=" crossorigin=anonymous></script><script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script><script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script><script src=https://tsumli.github.io/js/table-of-contents.js></script></body></html>