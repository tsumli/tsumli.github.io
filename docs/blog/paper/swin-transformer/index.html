<!DOCTYPE html>
<html lang="ja">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む | tsumli-pages</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  

  <link rel="shortcut icon" type="image/png" href="http://localhost:1313/favicon.ico" />


  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="http://localhost:1313/css/post.min.56c8810b0f07bc4b6fe3dd802a8194d08a2ed5a4903bdf2a4859c17f3860a7e7.css" integrity="sha256-VsiBCw8HvEtv492AKoGU0Iou1aSQO98qSFnBfzhgp&#43;c="/>
  
    
    <link type="text/css" rel="stylesheet" href="http://localhost:1313/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/"
      },
      "articleSection" : "blog",
      "name" : "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む",
      "headline" : "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2021",
      "datePublished": "2021-10-14 13:55:54 \u002b0000 UTC",
      "dateModified" : "2021-10-14 13:55:54 \u002b0000 UTC",
      "url" : "http:\/\/localhost:1313\/blog\/paper\/swin-transformer\/",
      "wordCount" : "1515",
      "keywords" : ["transformer", "Blog"]
    }
  
  </script>

</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="http://localhost:1313/">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="http://localhost:1313/blog">blog</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む</h1>
            <time datetime="2021-10-14 13:55:54 &#43;0000 UTC" class="post__date">Oct 14 2021</time> 
            <link rel="stylesheet" href="http://localhost:1313//css/lightbox.min.css">
            <script type="text/javascript">
 MathJax = {
   tex: {
     inlineMath: [['$','$'], ['\\(','\\)']],
     processEscapes: true,
     tags: "ams",
     autoload: {
       color: [],
       colorV2: ['color']
     },
     packages: {'[+]': ['noerrors']}
   },
   chtml: {
     matchFontHeight: false,
     displayAlign: "left", 
     displayIndent: "2em"
   },
   options: {
     skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
     renderActions: {
        
       find_script_mathtex: [10, function (doc) {
         for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
           const display = !!node.type.match(/; *mode=display/);
           const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
           const text = document.createTextNode('');
           node.parentNode.replaceChild(text, node);
           math.start = {node: text, delim: '', n: 0};
           math.end = {node: text, delim: '', n: 0};
           doc.math.push(math);
         }
       }, '']
     }
   },
   loader: {
     load: ['[tex]/noerrors']
   }
 };
</script>
<script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/mathjax-style.css">

          </header>
          <article class="post__content">
              
<p><a href="https://arxiv.org/abs/2103.14030" 
  
   target="_blank" rel="noreferrer noopener" 
><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</strong></a>
という論文を読んでいきます (本文中の図は論文より引用) 。</p>





<figure
  
  
  
  style='margin:0 auto;text-align:center;'
  >
    <a 
      
        data-lightbox="image-images/compare_vit.png"
      
      
        href="http://localhost:1313/blog/paper/swin-transformer/images/compare_vit.png"
      
    
    >
  <img
      
        src="http://localhost:1313/blog/paper/swin-transformer/images/compare_vit.png"
      
        
        
         />
    </a>
  
  
</figure>




<h2 id="method">Method<a class="anchor" href="#method">#</a></h2>
<p>
  





<figure
  
  
  
  style='margin:0 auto;text-align:center;'
  >
    <a 
      
        data-lightbox="image-images/architecture.png"
      
      
        href="http://localhost:1313/blog/paper/swin-transformer/images/architecture.png"
      
    
    >
  <img
      
        src="http://localhost:1313/blog/paper/swin-transformer/images/architecture.png"
      
        alt="The architecture of a Swin Transformer (b) two successive Swin Transformer Blocks."
        
        
         />
    </a>
  
  
    <figcaption>
      <span class="img--caption">
        Figure 1. The architecture of a Swin Transformer (b) two successive Swin Transformer Blocks.
        
      </span>
    </figcaption>
  
</figure>




Swin Transformerの全体像 (小さいバージョン)。
まず、RGB画像をオーバーラップしないようにpatchに分ける。そして、そのRGB値をconcatしたものが特徴量として扱われる。
この論文では、$4\times 4$ のpatchに分けている。つまり、 特徴量は $4\times 4 \times 3 = 48$ 次元となる。
この特徴量は線形層に通され、任意の次元に投影される。</p>
<h4 id="swin-transformer-block">Swin Transformer Block</h4>
<p>Multi-head self attention (MSA) をshifted-windowsのTransformerに置き換えることで作られる (図1 (b) 参照) 。</p>
<h3 id="shifted-window-based-self-attention">Shifted Window based Self-Attention<a class="anchor" href="#shifted-window-based-self-attention">#</a></h3>
<h4 id="shifted-window-partitioning-in-successive-blocks">Shifted window partitioning in successive blocks</h4>
<p>window-basedなself-attentionはwindow間をまたいで情報を得ることができない。
そこで、shifted windowの分け方を提案する。</p>
<p>下図にshifted windowのやり方を表す。
<br>
<br></p>

  





<figure
  
  
  
  style='margin:0 auto;text-align:center;'
  >
    <a 
      
        data-lightbox="image-images/shifted_window.png"
      
      
        href="http://localhost:1313/blog/paper/swin-transformer/images/shifted_window.png"
      
    
    >
  <img
      
        src="http://localhost:1313/blog/paper/swin-transformer/images/shifted_window.png"
      
        alt="Shifted window."
        
        
         />
    </a>
  
  
    <figcaption>
      <span class="img--caption">
        Figure 2. Shifted window.
        
      </span>
    </figcaption>
  
</figure>




<p>最初のモジュールは通常の分け方、つまり、左上から切り分けていく。
つまり $8 \times 8$ の特徴量を $4\times 4$ のサイズを持つwindowに切り分ける
(つまり、$2\times 2$のwindowが生成される) 。その次のモジュールではwindowが
$(\lfloor \frac{M}{2} \rfloor, \lfloor \frac{M}{2} \rfloor)$ だけ移動する (shifted-window)。
つまり、Swin Transformer blockでは下式のように計算される。</p>
<p>\begin{align*}
\hat{\mathbf{z}}^l &amp;= \text{W-MSA} (\mathrm{LN}(\mathbf{z}^{l-1})) + \mathbf{z}^{l-1}, \\
\mathbf{z}^l &amp;= \mathrm{MLP} (\mathrm{LN}(\hat{\mathbf{z}}^{l})) + \hat{\mathbf{z}}^{l}, \\
\hat{\mathbf{z}}^{l+1} &amp;= \text{SW-MSA} (\mathrm{LN}(\mathbf{z}^{l})) + \mathbf{z}^{l}, \\
\mathbf{z}^{l+1} &amp;= \mathrm{MLP} (\mathrm{LN}(\hat{\mathbf{z}}^{l+1})) + \hat{\mathbf{z}}^{l+1},
\end{align*}</p>
<h4 id="efficient-batch-computation-for-shifted-configuration">Efficient batch computation for shifted configuration</h4>

  





<figure
  
  
  
  style='margin:0 auto;text-align:center;'
  >
    <a 
      
        data-lightbox="image-images/batch_computation.png"
      
      
        href="http://localhost:1313/blog/paper/swin-transformer/images/batch_computation.png"
      
    
    >
  <img
      
        src="http://localhost:1313/blog/paper/swin-transformer/images/batch_computation.png"
      
        alt="Illustration of an efficient batch computation."
        
        
         />
    </a>
  
  
    <figcaption>
      <span class="img--caption">
        Figure 3. Illustration of an efficient batch computation.
        
      </span>
    </figcaption>
  
</figure>




<p>shifted windowの問題点として、windowの数が増える ($\lceil \frac{h}{M} \rceil \times\lceil \frac{w}{M} \rceil$ $\rightarrow$ $(\lceil \frac{h}{M} \rceil + 1) \times (\lceil \frac{w}{M} \rceil + 1)$) ことと、
いくつかのwindowのサイズが $M\times M$ よりも小さくなってしまうことが挙げられる。</p>
<p>これをナイーブな方法 (padding) ではなく解決するために、cyclic-shiftingを用いる (図3)。
このshiftを行ったあと、windowには隣接していない部分が含まれることになるが、maskingをすることでself-attentionに制限をかける。
この手法により、普通に分割したときと同じ数・サイズのwindowが生成できる。</p>
<h4 id="relative-position-bias">Relative position bias</h4>
<p>self-attentionを計算するとき、相対的なposition bias $B \in \mathbb{R}^{M^2 \times M^2}$を加える。
ここで、$M^2$ はwindowに含まれるpatchの数を表す。</p>
<p>つまり、次式でattentionの計算を行う
\begin{align*}
\text{Attention}(Q, K, V) = \text{SoftMax}(QK^\mathsf{T} / \sqrt{d} + B)V
\end{align*}</p>
<p>相対的な位置は $[-M+1, M-1]$ の範囲に収まるため、bias matrixを $\hat{B} \in \mathbb{R}^{(2M -1) \times (2M-1)}$ をのようにパラメータ化できる。
$B$の値は $\hat{B}$ から引っ張ってくる。</p>
<h3 id="architecture-variants">Architecture Variants<a class="anchor" href="#architecture-variants">#</a></h3>
<p>サイズと計算量のことなる複数のモデルを作ることができる。
ここで、Swin-T、Swin-Sはそれぞれ ResNet-50、ResNet-101に近いサイズである。</p>
<ul>
<li>Swin-T: $C=96$, layer numbers = {2, 2, 6, 2}</li>
<li>Swin-S: $C=96$, layer numbers = {2, 2, 18, 2}</li>
<li>Swin-B: $C=128$, layer numbers = {2, 2, 18, 2}</li>
<li>Swin-L: $C=192$, layer numbers = {2, 2, 18, 2}</li>
</ul>
<p>ここで、$C$ は最初のステージの隠れ層のチャンネルの数。</p>
<h2 id="experiment">Experiment<a class="anchor" href="#experiment">#</a></h2>
<h3 id="imagenet-1k">ImageNet-1K<a class="anchor" href="#imagenet-1k">#</a></h3>
<p>図4に、(a) 通常の方法で ImageNet-1Kで学習を行ったときと (b) ImageNet-22Kを使って事前学習を行ったときの結果を示す。

  





<figure
  
  
  
  style='margin:0 auto;text-align:center;'
  >
    <a 
      
        data-lightbox="image-images/imagenet-1K.png"
      
      
        href="http://localhost:1313/blog/paper/swin-transformer/images/imagenet-1K.png"
      
    
    >
  <img
      
        src="http://localhost:1313/blog/paper/swin-transformer/images/imagenet-1K.png"
      
        alt="imagenet-1K classificationに対する異なるモデルの比較。"
        
        
         />
    </a>
  
  
    <figcaption>
      <span class="img--caption">
        Figure 4. imagenet-1K classificationに対する異なるモデルの比較。
        
      </span>
    </figcaption>
  
</figure>



</p>
<p>(a) について、SOTA手法である ConvNets (RegNet) や EfficientNet と比べて Swin Transformerは速度と精度のよいトレードオフ (小さくはあるが) を達成していると言える。
(b) について、事前学習を行ったとき比較手法と比べ、提案手法はとても良いトレードオフを達成している。
他にも、COCO (Object Detection) や ADE (Semantic Segmentation) についても実験を行っている (SOTAなパフォーマンスを出している)。また、それぞれのモジュールについてablation studyが行われている。</p>
<h2 id="references">References<a class="anchor" href="#references">#</a></h2>
<ul>
<li>
<p>Swin Transformer implementation
<a href="https://github.com/microsoft/Swin-Transformer" 
  
   target="_blank" rel="noreferrer noopener" 
><img src="https://gh-card.dev/repos/microsoft/Swin-Transformer.svg" alt="microsoft/Swin-Transformer - GitHub"></a>
</p>
</li>
<li>
<p><a href="http://localhost:1313/blog/paper/video-swin-transformer/" 
  
  
>Video Swin Transformerのpost</a>
</p>
</li>
</ul>


              
                  

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>
              
          </article>
          

<ul class="tags__list">
    
    <li class="tag__item">
        <a class="tag__link" href="http://localhost:1313/tags/transformer/">transformer</a>
    </li>
    <li class="tag__item">
        <a class="tag__link" href="http://localhost:1313/tags/paper/">paper</a>
    </li></ul>

 <div class="pagination">
  
    <a class="pagination__item" href="http://localhost:1313/blog/paper/one-shot-talking-head/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing を読む</span>
    </a>
  

  
    <a class="pagination__item" href="http://localhost:1313/blog/paper/video-swin-transformer/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Video Swin Transformer を読む</span>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a class="social-icons__link" rel="me" title="Kaggle"
         href="https://www.kaggle.com/tsumli"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('http://localhost:1313/svg/kaggle.svg')"></div>
      </a>
    
  
     
    
      <a class="social-icons__link" rel="me" title="GitHub"
         href="https://github.com/tsumli"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('http://localhost:1313/svg/github.svg')"></div>
      </a>
    
     
</div>

            <p>© 2025</p>
            <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
            <link rel="stylesheet" href="http://localhost:1313//css/lightbox.min.css">
            <script src="http://localhost:1313//js/lightbox.min.js"></script>
          </footer>
          </div>
      </div>
      
      <div class="toc-container">
           <div class="toc-post-title">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows を読む</div> 
        <nav id="TableOfContents">
  <ul>
    <li><a href="#method">Method</a>
      <ul>
        <li></li>
        <li><a href="#shifted-window-based-self-attention">Shifted Window based Self-Attention</a></li>
        <li><a href="#architecture-variants">Architecture Variants</a></li>
      </ul>
    </li>
    <li><a href="#experiment">Experiment</a>
      <ul>
        <li><a href="#imagenet-1k">ImageNet-1K</a></li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="http://localhost:1313/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js" integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  
    <script src="http://localhost:1313/js/table-of-contents.js"></script>
  


</body>

</html>
