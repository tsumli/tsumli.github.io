<!doctype html><html lang=ja><head><title>CUDA: Cooperative Groupsについて | tsumli-pages</title><meta charset=UTF-8><meta name=language content="en"><meta name=description content><meta name=keywords content="cuda"><meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel="shortcut icon" type=image/png href=https://tsumli.github.io/favicon.ico><link type=text/css rel=stylesheet href=https://tsumli.github.io/css/post.min.56c8810b0f07bc4b6fe3dd802a8194d08a2ed5a4903bdf2a4859c17f3860a7e7.css integrity="sha256-VsiBCw8HvEtv492AKoGU0Iou1aSQO98qSFnBfzhgp+c="><link type=text/css rel=stylesheet href=https://tsumli.github.io/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css integrity="sha256-47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU="><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/tsumli.github.io\/"},"articleSection":"blog","name":"CUDA: Cooperative Groupsについて","headline":"CUDA: Cooperative Groupsについて","description":"","inLanguage":"en-US","author":"","creator":"","publisher":"","accountablePerson":"","copyrightHolder":"","copyrightYear":"2024","datePublished":"2024-08-02 00:55:54 \u002b0000 UTC","dateModified":"2024-08-02 00:55:54 \u002b0000 UTC","url":"https:\/\/tsumli.github.io\/blog\/cuda\/cooperative-groups\/","wordCount":"3976","keywords":["cuda","Blog"]}</script></head><body><div class=burger__container><div class=burger aria-controls=navigation aria-label=Menu><div class="burger__meat burger__meat--1"></div><div class="burger__meat burger__meat--2"></div><div class="burger__meat burger__meat--3"></div></div></div><nav class=nav id=navigation><ul class=nav__list><li><a href=https://tsumli.github.io/>about</a></li><li><a class=active href=https://tsumli.github.io/blog>blog</a></li></ul></nav><main><div class=flex-wrapper><div class=post__container><div class=post><header class=post__header><h1 id=post__title>CUDA: Cooperative Groupsについて</h1><time datetime="2024-08-02 00:55:54 +0000 UTC" class=post__date>Aug 2 2024</time>
<link rel=stylesheet href=https://tsumli.github.io//css/lightbox.min.css><script type=text/javascript>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,tags:"ams",autoload:{color:[],colorV2:["color"]},packages:{"[+]":["noerrors"]}},chtml:{matchFontHeight:!1,displayAlign:"left",displayIndent:"2em"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],renderActions:{find_script_mathtex:[10,function(e){for(const t of document.querySelectorAll('script[type^="math/tex"]')){const o=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],o),s=document.createTextNode("");t.parentNode.replaceChild(s,t),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},e.math.push(n)}},""]}},loader:{load:["[tex]/noerrors"]}}</script><script type=text/javascript async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js id=MathJax-script></script><link rel=stylesheet type=text/css href=https://tsumli.github.io/css/mathjax-style.css></header><article class=post__content><p><a href=https://developer.nvidia.com/blog/cooperative-groups/ target=_blank rel="noreferrer noopener">NVIDIAのblog</a>
を読んでいたのですが良く分からなかったのでコードを動かしながら見ていきます。</p><h2 id=cooperative-groups-とは>Cooperative Groups とは?<a class=anchor href=#cooperative-groups-とは>#</a></h2><figure style='margin:0 auto;text-align:center'><a data-lightbox=image-images/overview.png href=https://tsumli.github.io/blog/cuda/cooperative-groups/images/overview.png><img src=https://tsumli.github.io/blog/cuda/cooperative-groups/images/overview.png alt="Concepts of Cooperative Groups."></a><figcaption><span class=img--caption>Figure . Concepts of Cooperative Groups.</span></figcaption></figure><p>今までは<code>__syncthreads()</code>を使わないと同期できませんでした。しかし、これよりも小さなグループで同期を取りたいことがあります。Cooperative GroupsはCUDA 9から導入された機能で、これを使うことによって柔軟な同期が可能になります。</p><p><code>cooperative_groups.h</code>をincludeするだけで使うことができます。そして、一般的に<code>cg</code>とaliasされることが多いようです。</p><pre><code class=language-cpp>#include &lt;cooperative_groups.h&gt;
namespace cg = cooperative_groups;
</code></pre><p>これより先のコードではこの定義を使うため注意してください。</p><h2 id=thread-group>Thread Group<a class=anchor href=#thread-group>#</a></h2><p>Cooperative Groupsの中でも最も基本の型は<code>thread_group</code>です。threadの数やthreadのindex ([0, size - 1]) は次のように取得できます。</p><pre><code class=language-cpp>__global__ void my_kernel() {
    cg::thread_block block = cg::this_thread_block();
    printf(&quot;size: %d, thread_rank: %d\n&quot;, block.size(), block.thread_rank());
}

int main() {
    my_kernel&lt;&lt;&lt;2, 4&gt;&gt;&gt;();
    THROW_IF_FAILED(cudaDeviceSynchronize());
    return 0;
}
</code></pre><p>出力は以下になります</p><pre><code class=language-txt>size: 4, thread_rank: 0
size: 4, thread_rank: 1
size: 4, thread_rank: 2
size: 4, thread_rank: 3
size: 4, thread_rank: 0
size: 4, thread_rank: 1
size: 4, thread_rank: 2
size: 4, thread_rank: 3
</code></pre><h2 id=thread-group-collective-operations>Thread Group Collective Operations<a class=anchor href=#thread-group-collective-operations>#</a></h2><p>Collective operationsは同期が必要だったり、特定のthreadとのコミュニケーションが必要な操作のことを指します。最も簡単な操作はグループ全体の同期です。これは<code>sync()</code>を使うことで可能になります。</p><pre><code class=language-cpp>g.sync();
cg::synchronize(g);
</code></pre><h2 id=thread-blocks>Thread Blocks<a class=anchor href=#thread-blocks>#</a></h2><p><code>thread_block</code>はkernelの中で thread block (= CUDA並列プログラミングにおける基本的な単位) を表す新しい型です。次のように初期化されます。</p><pre><code class=language-cpp>cg::thread_block block = cg::this_thread_block();
</code></pre><p>この<code>thread_block</code>グループの同期は<code>__syncthreads()</code>と同じになります。つまり、以下の5つは同じ操作になります。</p><pre><code class=language-cpp>__syncthreads();
block.sync();
cg::synchronize(block);
cg::this_thread_block().sync();
cg::synchronize(this_thread_block());
</code></pre><p><code>thread_block</code>は<code>thread_group</code>のインターフェースに加えblock特有の関数が追加されています。</p><pre><code class=language-cpp>dim3 group_index();  // 3-dimensional block index within the grid
dim3 thread_index(); // 3-dimensional thread index within the block
</code></pre><p>これはそれぞれCUDAでいうところの<code>blockIdx</code>と<code>threadIdx</code>です。</p><pre><code class=language-cpp>__global__ void my_kernel() {
    cg::thread_block block = cg::this_thread_block();
    printf(&quot;group_index: (%d, %d, %d), thread_index: (%d, %d, %d)\n&quot;, block.group_index().x,
           block.group_index().y, block.group_index().z, block.thread_index().x,
           block.thread_index().y, block.thread_index().z);
}

int main() {
    my_kernel&lt;&lt;&lt;2, 4&gt;&gt;&gt;();
    THROW_IF_FAILED(cudaDeviceSynchronize());
    return 0;
}
</code></pre><p>出力は次の通りです</p><pre><code class=language-md>group_index: (1, 0, 0), thread_index: (0, 0, 0)
group_index: (1, 0, 0), thread_index: (1, 0, 0)
group_index: (1, 0, 0), thread_index: (2, 0, 0)
group_index: (1, 0, 0), thread_index: (3, 0, 0)
group_index: (0, 0, 0), thread_index: (0, 0, 0)
group_index: (0, 0, 0), thread_index: (1, 0, 0)
group_index: (0, 0, 0), thread_index: (2, 0, 0)
group_index: (0, 0, 0), thread_index: (3, 0, 0)
</code></pre><p>(疑問) <code>blockIdx</code>/<code>threadIdx</code>と<code>group_index</code>/<code>thread_idx</code>はどう使い分けるべき?</p><h2 id=partitioning-groups>Partitioning Groups<a class=anchor href=#partitioning-groups>#</a></h2><p>ここからがCooperative Groupsの本領発揮といったところでしょうか。groupを分割する機能についてみていきます。<code>cg::tiled_partition()</code>でblockを複数の"tile"に分割することができます。以下の例は、thread blockを32個のthreadを持つtileに分割する例です。</p><pre><code class=language-cpp>cg::thread_group tile32 = cg::tiled_partition(cg::this_thread_block(), 32);
</code></pre><p>それぞれのthreadが32-threadのグループへのhandleを得ます。さらに4-threadのtileに分割してみましょう</p><pre><code class=language-cpp>cg::thread_group tile4 = cg::tiled_partition(tile32, 4);
</code></pre><p><code>tiled_partition()</code>によって作成されたthread groupも他のthread group同様の振る舞いをします。ここで、次の例を見てみましょう。</p><pre><code class=language-cpp>__global__ void my_kernel() {
    cg::thread_block block = cg::this_thread_block();
    cg::thread_group tile4 = cg::tiled_partition(block, 4);

    if (tile4.thread_rank() == 0) {
        printf(&quot;Hello from tile4 rank 0: %d\n&quot;, block.thread_rank());
    }
}

int main() {
    my_kernel&lt;&lt;&lt;2, dim3{2, 16, 1}&gt;&gt;&gt;();
    THROW_IF_FAILED(cudaDeviceSynchronize());
    return 0;
}
</code></pre><p>出力は次の通り</p><pre><code>Hello from tile4 rank 0: 0
Hello from tile4 rank 0: 4
Hello from tile4 rank 0: 8
Hello from tile4 rank 0: 12
Hello from tile4 rank 0: 16
Hello from tile4 rank 0: 20
Hello from tile4 rank 0: 24
Hello from tile4 rank 0: 28
Hello from tile4 rank 0: 0
Hello from tile4 rank 0: 4
Hello from tile4 rank 0: 8
Hello from tile4 rank 0: 12
Hello from tile4 rank 0: 16
Hello from tile4 rank 0: 20
Hello from tile4 rank 0: 24
Hello from tile4 rank 0: 28
</code></pre><p>tile4は4つのthreadごとに<code>thread_rank()</code>が0になることがわかります</p><h2 id=modularity>Modularity<a class=anchor href=#modularity>#</a></h2><p>さまざまなthread groupのサイズに対して一貫したインターフェースを利用できるため、race conditionやdeadlockを防ぐことができます。</p><p>例えば、次のようなコードではdeadlockが発生します。</p><pre><code class=language-cpp>__device__ int sum(int *x, int n) 
{
    ...
    __syncthreads();
    ...
    return total;
}

__global__ void parallel_kernel(float *x, int n)
{
    if (threadIdx.x &lt; blockDim.x / 2) {
        sum(x, count);  // error: half of threads in block skip
                        // __syncthreads() =&gt; deadlock
    }
}
</code></pre><p>しかし、<code>thread_block</code>を引数に取ることでこのようなdeadlockを防ぐことができます。</p><pre><code class=language-cpp>// Now much clearer that a whole thread block is expected to call
__device__ int sum(cg::thread_block block, int *x, int n) 
{
    ...
    block.sync();
    ...
    return total;
}

__global__ void parallel_kernel(float *x, int n)
{
    sum(cg::this_thread_block(), x, count); // no divergence around call
}
</code></pre><p>以下は32 threadのtileを使用したsum関数の例です。tileごとに合計を計算し、最終的に<code>atomicAdd</code>で足し合わされます。</p><pre><code class=language-cpp>__global__ void sum_kernel_32(int *sum, int *input, int n)
{
    int my_sum = thread_sum(input, n); 

    extern __shared__ int temp[];

    auto g = cg::this_thread_block();
    auto tileIdx = g.thread_rank() / 32;
    int* t = &amp;temp[32 * tileIdx];
    
    auto tile32 = cg::tiled_partition(g, 32);  
    int tile_sum = reduce_sum(tile32, t, my_sum);

    if (tile32.thread_rank() == 0) {
        atomicAdd(sum, tile_sum);
    }
}
</code></pre><p>(疑問) 上の方法だと1つのthreadごとに<code>atomicAdd</code>するよりも速くなる?</p><h2 id=optimizing-for-the-gpu-warp-size>Optimizing for the GPU Warp Size<a class=anchor href=#optimizing-for-the-gpu-warp-size>#</a></h2><p><code>cg::tiled_partition()</code>はtemplate parameterをとるバージョンもあります</p><pre><code class=language-cpp>cg::thread_block_tile&lt;32&gt; tile32 = cg::tiled_partition&lt;32&gt;(this_thread_block());
cg::thread_block_tile&lt;4&gt;  tile4  = cg::tiled_partition&lt;4&gt; (this_thread_block());
</code></pre><p>これを使用することでreductionのコードを少し最適化することができます。</p><pre><code class=language-cpp>template &lt;typename group_t&gt;
__device__ int reduce_sum(group_t g, int *temp, int val)
{
    int lane = g.thread_rank();

    // Each iteration halves the number of active threads
    // Each thread adds its partial sum[i] to sum[lane+i]
    #pragma unroll
    for (int i = g.size() / 2; i &gt; 0; i /= 2)
    {
        temp[lane] = val;
        g.sync(); // wait for all threads to store
        if (lane &lt; i) val += temp[lane + i];
        g.sync(); // wait for all threads to load
    }

    return val; // note: only thread 0 will return full sum
}
</code></pre><p>また、tileのサイズがwarpのサイズと一致した時、compilerは同期を省略できる場合があります。熟練のプログラマがwarpを考慮した上で同期を取り除くことがよく行われるが、thread groupを明示的に同期するようなコードを書くことでrace conditionの発生を防ぐことができます (compilerが最適化してくれる?) 。</p><h2 id=warp-level-collectives>Warp-Level Collectives<a class=anchor href=#warp-level-collectives>#</a></h2><p>thread block tilesは次のwarp-levelなcollective functionをもつ。</p><pre><code class=language-cpp>.shfl()
.shfl_down()
.shfl_up()
.shfl_xor()
.any()
.all()
.ballot()
.match_any()
.match_all()
</code></pre><p>ここで、<code>shfl_down()</code>を使用したreductionのコードをみてみましょう。このコードではshared memoryを使用していません。</p><pre><code class=language-cpp>__device__ int thread_sum(int *input, int n) {
    int sum = 0;
    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n / 4; i += blockDim.x * gridDim.x) {
        int4 in = ((int4 *)input)[i];
        sum += in.x + in.y + in.z + in.w;
    }
    return sum;
}

template &lt;int tile_sz&gt;
__device__ int reduce_sum_tile_shfl(cg::thread_block_tile&lt;tile_sz&gt; g, int val) {
    // Each iteration halves the number of active threads
    // Each thread adds its partial sum[i] to sum[lane+i]
    for (int i = g.size() / 2; i &gt; 0; i /= 2) {
        val += g.shfl_down(val, i);
    }

    return val;  // note: only thread 0 will return full sum
}

template &lt;int tile_sz&gt;
__global__ void sum_kernel_tile_shfl(int *sum, int *input, int n) {
    int my_sum = thread_sum(input, n);

    auto tile = cg::tiled_partition&lt;tile_sz&gt;(cg::this_thread_block());
    int tile_sum = reduce_sum_tile_shfl&lt;tile_sz&gt;(tile, my_sum);

    if (tile.thread_rank() == 0) {
        atomicAdd(sum, tile_sum);
    }
}
</code></pre><h2 id=discovering-thread-concurrency>Discovering Thread Concurrency<a class=anchor href=#discovering-thread-concurrency>#</a></h2><p>GPUでは32個のthreadからなるwarp単位でthread命令を実行します。コード内の条件分岐によってwarp内のthreadが分岐すると、アクティブなthreadとそうでないthreadに分かれます。アクティブなままでいるthreadは<code>coalesced</code>と呼ばれます。Cooperative Groupsではこのようなthreadのgroupを取得するための関数を提供しています。</p><pre><code class=language-cpp>cg::coalesced_group active = cg::coalesced_threads();
</code></pre><p>例えば、次のような奇数のthread rankをもつthreadのみで計算を行う場合、アクティブなthreadのみに対して同期を実行できる。しかし、coalesced threadはwarpを跨ぐことはないため注意が必要である。</p><pre><code class=language-cpp>auto block = cg::this_thread_block();

if (block.thread_rank() % 2) {
    cg::coalesced_group active = cg::coalesced_threads();
    ...
    active.sync();
}
</code></pre><p>良い例として"warp-aggregation atomics"を考えてみましょう。warp-aggregationではwarpのthreadが増加分を合計したあと、その中の1つのthreadがグローバル変数のカウンタにアトミックに加算を行います。これによってatomic演算がwarpのサイズ分だけ減り、パフォーマンスがよくなります。</p><p>しかし、ここでキーポイントとなるのは、warpの最初のthreadでアトミック演算を行おうとしても、それがアクティブなthreadでない可能性があるということです。ここで、<code>coalesced_group</code>の<code>thread_rank()</code>を使うことができます。</p><pre><code class=language-cpp>__device__ int atomicAggInc(int *ptr) {
    cg::coalesced_group g = cg::coalesced_threads();
    int prev;

    // elect the first active thread to perform atomic add
    if (g.thread_rank() == 0) {
        prev = atomicAdd(ptr, g.size());
    }

    // broadcast previous value within the warp
    // and add each active thread’s rank to it
    prev = g.thread_rank() + g.shfl(prev, 0);
    return prev;
}
</code></pre><h2 id=実験>実験<a class=anchor href=#実験>#</a></h2><p>さて、ここまでブログの記事をざっくりとみていきましたが、実際にコードを動かしてみましょう (今までもいくつか動かしていましたが&mldr;)
<code>cg::thread_block</code>から取得できる情報をみていきます。一気に出力すると良く分からなくなるのでコメントアウトしながら動かします。</p><pre><code class=language-cpp>__global__ void my_kernel() {
    cg::thread_block block = cg::this_thread_block();

    dim3 dim_threads = block.dim_threads();
    uint num_threads = block.num_threads();
    uint get_type = block.get_type();
    dim3 group_dim = block.group_dim();
    dim3 group_index = block.group_index();
    uint thread_rank = block.thread_rank();
    uint size = block.size();

    printf(&quot;dim_threads: (%d, %d, %d)\n&quot;, dim_threads.x, dim_threads.y, dim_threads.z);
    // printf(&quot;num_threads: %d\n&quot;, num_threads);
    // printf(&quot;get_type: %d\n&quot;, get_type);
    // printf(&quot;group_dim: (%d, %d, %d)\n&quot;, group_dim.x, group_dim.y, group_dim.z);
    // printf(&quot;group_index: (%d, %d, %d)\n&quot;, group_index.x, group_index.y, group_index.z);
    // printf(&quot;thread_index: (%d, %d, %d)\n&quot;, thread_index.x, thread_index.y, thread_index.z);
    // printf(&quot;thread_rank: %d\n&quot;, thread_rank);
    // printf(&quot;size: %d\n&quot;, size);
}

int main() {
    my_kernel&lt;&lt;&lt;dim3{2, 3, 4}, dim3{5, 6, 7}&gt;&gt;&gt;();
    THROW_IF_FAILED(cudaDeviceSynchronize());
    return 0;
}
</code></pre><h3 id=dim_threads><code>dim_threads</code><a class=anchor href=#dim_threads>#</a></h3><pre><code>dim_threads: (5, 6, 7)
dim_threads: (5, 6, 7)
dim_threads: (5, 6, 7)
...
</code></pre><p><code>dim3{5, 6, 7}</code>を返していますね</p><h3 id=num_threads><code>num_threads</code><a class=anchor href=#num_threads>#</a></h3><pre><code>num_threads: 210
num_threads: 210
num_threads: 210
...
</code></pre><p>$5 \times 6 \times 7 = 210$個のthreadが実行されています</p><h3 id=get_type><code>get_type</code><a class=anchor href=#get_type>#</a></h3><pre><code>get_type: 4
get_type: 4
get_type: 4
...
</code></pre><p>このtypeは以下の値に該当するようです。つまり、このblockのtypeは<code>thread_block_id</code>であることがわかります。</p><pre><code class=language-cpp>// cooperative_group.h
namespace details {
    _CG_CONST_DECL unsigned int coalesced_group_id = 1;
    _CG_CONST_DECL unsigned int multi_grid_group_id = 2;
    _CG_CONST_DECL unsigned int grid_group_id = 3;
    _CG_CONST_DECL unsigned int thread_block_id = 4;
    _CG_CONST_DECL unsigned int multi_tile_group_id = 5;
    _CG_CONST_DECL unsigned int cluster_group_id = 6;
}
</code></pre><h3 id=group_dim><code>group_dim</code><a class=anchor href=#group_dim>#</a></h3><pre><code>group_dim: (5, 6, 7)
group_dim: (5, 6, 7)
group_dim: (5, 6, 7)
...
</code></pre><p>これも<code>dim3{5, 6, 7}</code>を返しています。</p><h3 id=group_index-thread_idx><code>group_index</code>, <code>thread_idx</code><a class=anchor href=#group_index-thread_idx>#</a></h3><pre><code>...
group_index: (0, 0, 1)
group_index: (0, 0, 1)
group_index: (0, 0, 0)
...
</code></pre><pre><code>...
thread_index: (4, 3, 0)
thread_index: (0, 4, 0)
thread_index: (1, 4, 0)
...
</code></pre><p>こちらについては先ほど紹介した通り<code>blockIdx</code>/<code>threadIdx</code>と同じですね</p><h3 id=size><code>size</code><a class=anchor href=#size>#</a></h3><pre><code>...
size: 210
size: 210
size: 210
...
</code></pre><p>これは$5 \times 6 \times 7$と一致します。</p><h3 id=thread_rank><code>thread_rank</code><a class=anchor href=#thread_rank>#</a></h3><pre><code>...
thread_rank: 189
thread_rank: 190
thread_rank: 191
thread_rank: 64
thread_rank: 65
thread_rank: 66
...
</code></pre><p>この値は0から209まで取るかと思ったのですが、調べてみると0から191までのようです。6つ分のwarpのみ動いているということでしょうか。ここで気がついたのですが、標準出力の行数も4608 ($= 2 * 3 * 4 * 192$) となっていました。192から209番目のthreadは実行されていないのは、今回のコードがただprintfするコードのためcompilerによる最適化がかかったと考えられます。</p><p>続きは余力があれば&mldr;</p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script></article><ul class=tags__list><li class=tag__item><a class=tag__link href=https://tsumli.github.io/tags/cuda/>cuda</a></li></ul><div class=pagination><a class=pagination__item href=https://tsumli.github.io/blog/personal/desktop-20231201/><span class=pagination__label>Previous Post</span>
<span class=pagination__title>新しいPCを組んだので構成など</span></a></div><footer class=post__footer><div class=social-icons><a class=social-icons__link rel=me title=Kaggle href=https://www.kaggle.com/tsumli target=_blank rel=noopener><div class=social-icons__icon style=background-image:url(https://tsumli.github.io/svg/kaggle.svg)></div></a><a class=social-icons__link rel=me title=GitHub href=https://github.com/tsumli target=_blank rel=noopener><div class=social-icons__icon style=background-image:url(https://tsumli.github.io/svg/github.svg)></div></a></div><p>© 2025</p><script src=https://code.jquery.com/jquery-3.4.1.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><link rel=stylesheet href=https://tsumli.github.io//css/lightbox.min.css><script src=https://tsumli.github.io//js/lightbox.min.js></script></footer></div></div><div class=toc-container><div class=toc-post-title>CUDA: Cooperative Groupsについて</div><nav id=TableOfContents><ul><li><a href=#cooperative-groups-とは>Cooperative Groups とは?</a></li><li><a href=#thread-group>Thread Group</a></li><li><a href=#thread-group-collective-operations>Thread Group Collective Operations</a></li><li><a href=#thread-blocks>Thread Blocks</a></li><li><a href=#partitioning-groups>Partitioning Groups</a></li><li><a href=#modularity>Modularity</a></li><li><a href=#optimizing-for-the-gpu-warp-size>Optimizing for the GPU Warp Size</a></li><li><a href=#warp-level-collectives>Warp-Level Collectives</a></li><li><a href=#discovering-thread-concurrency>Discovering Thread Concurrency</a></li><li><a href=#実験>実験</a><ul><li><a href=#dim_threads><code>dim_threads</code></a></li><li><a href=#num_threads><code>num_threads</code></a></li><li><a href=#get_type><code>get_type</code></a></li><li><a href=#group_dim><code>group_dim</code></a></li><li><a href=#group_index-thread_idx><code>group_index</code>, <code>thread_idx</code></a></li><li><a href=#size><code>size</code></a></li><li><a href=#thread_rank><code>thread_rank</code></a></li></ul></li></ul></nav></div></div></main><script src=https://tsumli.github.io/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin=anonymous></script><script src=https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js></script><script src=https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js data-autoloader-path=https://unpkg.com/prismjs@1.20.0/components/></script><script src=https://tsumli.github.io/js/table-of-contents.js></script></body></html>